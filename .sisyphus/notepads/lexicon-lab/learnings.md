# Lexicon-Lab Learnings

## Basic CLI Framework (Completed)

### Files Created
1. **lexicon/__init__.py** - Package initialization with version and author metadata
2. **lexicon/cli.py** - Main CLI entry point with typer framework

### Key Learnings
- Typer simplifies CLI development with decorator-based command registration
- All 8 placeholder commands (search, chain, define, synonym, antonym, random, stats, init) are working
- Entry point configured in pyproject.toml as `lexicon = "lexicon.cli:app"`
- Commands are callable as `lexicon <command>`

### pyproject.toml Fix
- Fixed `packages` config from `["."]` to `["lexicon"]` to comply with setuptools validation

### CLI Structure
- Main app: `typer.Typer()` with description
- Commands decorated with `@app.command()`
- Each command returns simple "Not implemented yet" message
- Help system automatically generated by typer

### Commands Implemented
- `lexicon search` - Search for words
- `lexicon chain` - Word chaining game
- `lexicon define` - Word definition lookup
- `lexicon synonym` - Find synonyms
- `lexicon antonym` - Find antonyms
- `lexicon random` - Get random words
- `lexicon stats` - Show statistics
- `lexicon init` - Initialize word database

### Testing
- All commands tested and responding correctly
- Help system working: `lexicon --help`
- Individual command help: `lexicon <command> --help`

## LexiconIndex Implementation

### Class Structure
- Created `lexicon/index.py` with `LexiconIndex` class
- All indexes use `dict[str|int, list[int]]` mapping to word indices
- Uses `defaultdict(list)` during build, then converts to regular dicts
- Statistics use `Counter` from collections

### Index Types Built
1. **Character-based**: by_first_char, by_last_char, by_char
2. **Pinyin-based**: by_pinyin_initials, by_pinyin_no_tone
3. **Rhyme-based**: by_rhyme
4. **Metadata-based**: by_length, by_category, by_structure
5. **Frequency counters**: char_freq_start, char_freq_end, char_freq_all

### Implementation Pattern
- Single pass through words list in `_build_indexes()`
- Efficient O(1) lookups after index building
- Handles optional fields (structure) with conditional indexing
- Type hints for all attributes and methods

### Code Quality
- Removed all unnecessary inline comments
- Kept only essential public API docstrings (module, class, __init__)
- Private method `_build_indexes` is self-documenting without docstring
- Clean, readable code with descriptive variable names


## Pinyin Processing with Multi-Pronunciation Support (2025-01-31)

### Implementation Details
Created `lexicon/pinyin_utils.py` with comprehensive pinyin processing utilities:

1. **Core Functions**:
   - `get_pinyin(word)` - Full pinyin with tones (space-separated)
   - `get_pinyin_no_tone(word)` - Pinyin without tones
   - `get_pinyin_initials(word)` - **ALL possible initial combinations** for multi-pronunciation chars
   - `get_tones(pinyin_str)` - Extract tone sequence (e.g., "1,2,3,4")
   - `get_rhyme(word)` - Get rhyme (éŸµæ¯) of last character

2. **Multi-Pronunciation Handling**:
   - Used `pypinyin` library with `heteronym=True` parameter
   - Generates **Cartesian product** of all possible pronunciations
   - Example: "æœé˜³" â†’ initials ["zy", "cy"] for both "zhÄo yÃ¡ng" (morning sun) and "chÃ¡o yÃ¡ng" (district name)
   - Removes duplicates while preserving order

3. **Key Technical Choices**:
   - `Style.FIRST_LETTER` for initials extraction
   - `Style.TONE` for pinyin with tone marks
   - `Style.NORMAL` for pinyin without tones
   - `Style.FINALS` for rhyme (éŸµæ¯) extraction
   - `itertools.product()` for generating all pronunciation combinations

4. **Tone Mark Mapping**:
   - Created comprehensive Unicode tone mark dictionary
   - Maps Ä/Ã¡/Ç/Ã  â†’ 1/2/3/4 for all vowels (a, e, i, o, u, Ã¼)
   - Neutral tone (è½»å£°) represented as "0"

### Testing Results
- âœ… Multi-pronunciation: "æœé˜³" â†’ ["zy", "cy"] âœ“
- âœ… Single pronunciation: "ä¸­å›½" â†’ ["zg"] âœ“
- âœ… Tone extraction: "zhÄo yÃ¡ng" â†’ "1,2" âœ“
- âœ… Rhyme extraction: "æœé˜³" â†’ "iang" âœ“

### Patterns Learned
- **Cartesian Product for Multi-Pronunciation**: Use `itertools.product(*all_pinyins)` to generate all possible combinations
- **Heteronym Support**: Always use `heteronym=True` when dealing with å¤šéŸ³å­—
- **Duplicate Removal**: Use set for deduplication while maintaining list order


## SearchEngine Implementation (2025-02-01)

### Successfully Implemented
- Created `lexicon/search.py` with `SearchEngine` class
- Loads 61,069 total words from 3 JSON files:
  - 30,895 idioms (æˆè¯­) from `idiom.json`
  - 16,142 words (è¯è¯­) from `word.json`
  - 14,032 xiehouyu (æ­‡åè¯­) from `xiehouyu.json`

### JSON Field Mapping Strategy
**Idioms (`idiom.json`):**
- `word` â†’ word
- `explanation` â†’ definition
- `derivation` â†’ source
- `example` â†’ example
- `pinyin` field exists but we regenerate using pinyin_utils for consistency

**Words (`word.json`):**
- `word` â†’ word
- `explanation` â†’ definition (fallback to `more` if empty)
- No source or example fields (set to None)

**Xiehouyu (`xiehouyu.json`):**
- `riddle` â†’ word
- `answer` â†’ definition
- No source or example fields

### Pinyin Generation
All pinyin fields are generated using `pinyin_utils`:
- `get_pinyin()` â†’ pinyin (with tones)
- `get_pinyin_no_tone()` â†’ pinyin_no_tone
- `get_pinyin_initials()` â†’ pinyin_initials (uses first variant for multi-pronunciation chars)
- `get_tones()` â†’ tones
- `get_rhyme()` â†’ rhyme (last character's final)

### Performance
- Uses `orjson` for fast JSON parsing
- Loads all 61K words in ~3-5 seconds
- Robust error handling with logging for failed entries
- Progress logging for each data source

### Design Decisions
- Used `pathlib.Path` for cross-platform file handling
- Set `structure`, `synonyms`, `antonyms`, `frequency` to None (future work)
- Category field maps directly to Chinese names: "æˆè¯­", "è¯è¯­", "æ­‡åè¯­"
- Graceful handling of missing/empty fields with .get() and .strip()
- Continued loading even if individual entries fail (with warnings)

### Testing Results
âœ… All three categories load successfully
âœ… Pinyin generation works correctly
âœ… Character extraction (first_char, last_char, chars, length) accurate
âœ… Category assignment correct
âœ… Definition/source/example field mapping correct
âœ… Handles missing fields gracefully


## Search Method Implementation (2025-01-31)

Successfully added `search()` method to `SearchEngine` class with the following approach:

### Filter Strategy
Applied filters in order of selectivity for optimal performance:
1. Category filter (most selective - 3 categories only)
2. Length filter (very selective - limited range)
3. Pinyin initials filter
4. Full pinyin (no tone) filter
5. First character filter
6. Last character filter
7. Contains filter (intersect all chars)
8. Regex filter (must scan, no index available)

### Implementation Details
- Used set intersection (`&=`) for efficient filtering
- Early returns when candidates empty (short-circuit optimization)
- Regex compilation with error handling
- Index lookups via `dict.get(key, [])` for safety

### Key Design Decisions
- Start with all indices as candidates: `set(range(len(self.words)))`
- Apply most selective filters first to reduce candidate set quickly
- Regex filter applied last (no index support, must scan remaining)
- Limit applied at the end when converting indices to Word objects


## Search CLI Command Implementation (2025-02-01)

### Successfully Implemented
Updated `lexicon/cli.py` with full search command functionality:

### Implementation Details

1. **Global SearchEngine Instance (Lazy Loading)**:
   - Module-level `_search_engine: Optional[SearchEngine] = None`
   - `get_search_engine()` function creates instance on first call
   - Subsequent calls return cached instance
   - Ensures single SearchEngine instance across CLI commands

2. **Typer Options (All Required)**:
   - `-s, --start TEXT` - First character (é¦–å­—)
   - `-e, --end TEXT` - Last character (å°¾å­—)
   - `-c, --contains TEXT` - Characters that must be present (multiple allowed)
   - `-p, --pinyin TEXT` - Pinyin initials (æ‹¼éŸ³é¦–å­—æ¯)
   - `-P, --full-pinyin TEXT` - Full pinyin without tones (å®Œæ•´æ‹¼éŸ³)
   - `-r, --regex TEXT` - Regular expression pattern (æ­£åˆ™è¡¨è¾¾å¼)
   - `-l, --length INT` - Word length (è¯è¯­é•¿åº¦)
   - `-t, --category TEXT` - Category type (ç±»å‹: æˆè¯­/è¯è¯­/æ­‡åè¯­)
   - `--limit INT` - Default 20 (ç»“æœæ•°é‡)
   - `--no-pinyin` - Flag to hide pinyin (ä¸æ˜¾ç¤ºæ‹¼éŸ³)
   - `--no-definition` - Flag to hide definition (ä¸æ˜¾ç¤ºé‡Šä¹‰)

3. **Output Format**:
   - "æ‰¾åˆ° N æ¡ç»“æœ:" header with count
   - Numbered list: "1. word [pinyin] - definition"
   - Conditionally includes pinyin and definition based on flags
   - Uses `typer.echo()` for all output

4. **Output Examples**:
```
æ‰¾åˆ° 3 æ¡ç»“æœ:

1. é˜¿é¼»åœ°ç‹± [Ä bÃ­ dÃ¬ yÃ¹] - é˜¿é¼»æ¢µè¯­çš„è¯‘éŸ³...
2. é˜¿å…šæ¯”å‘¨ [Ä“ dÇng bÇ zhÅu] - æŒ‡ç›¸äº’å‹¾ç»“...
3. é˜¿å…šç›¸ä¸º [Ä“ dÇng xiÄng wÃ©i] - é˜¿å…šåè¢’...
```

With `--no-pinyin`:
```
1. é˜¿é¼»åœ°ç‹± - é˜¿é¼»æ¢µè¯­çš„è¯‘éŸ³...
```

With `--no-definition`:
```
1. é˜¿é¼»åœ°ç‹± [Ä bÃ­ dÃ¬ yÃ¹]
```

### Testing Coverage
âœ… All options work individually and in combination
âœ… Multiple `--contains` parameters supported
âœ… `--no-pinyin` flag suppresses pinyin display
âœ… `--no-definition` flag suppresses definition display
âœ… Filters: start, end, length, category, pinyin, regex all functional
âœ… Default limit=20 works as expected
âœ… Exit code 0 on success
âœ… Help system automatically generated by typer

### Code Quality
- Clean, self-documenting code (no unnecessary comments)
- Proper type hints: `Optional[str]`, `Optional[List[str]]`, `Optional[int]`, `bool`
- Efficient single pass through results for formatting
- Handles empty definitions gracefully (no trailing separator)

### Command Examples
```bash
lexicon search --category æˆè¯­ --limit 5
lexicon search --start åˆ« --end å¤©
lexicon search --contains å¤© --contains åœ° --no-pinyin
lexicon search --regex "^åˆ«" --no-definition
lexicon search --length 4 --category æˆè¯­
```

All tests pass and functionality is production-ready.

## Structure Detection Implementation (P2 - COMPLETED)

### Summary
Successfully implemented structure detection for Chinese words using regex backreferences.

### Key Learnings

#### 1. Raw String Backreferences in Python
- In regex patterns with backreferences, use raw strings: `r"^(.)\1(.)\2$"`
- **DO NOT** use `r"^(.)\\1(.)\\2$"` (this creates literal backslashes)
- In raw strings `r"\1"` produces the actual string needed by regex engine
- Python repr shows it as `\\1` (escaped for display), but it's correct

#### 2. Structure Pattern Order
Patterns are checked in order of specificity. Current implementation checks:
1. AABB - Same pairs repeated (é«˜é«˜å…´å…´)
2. ABAB - Different pairs repeated (ç ”ç©¶ç ”ç©¶)
3. ABAC - First and third chars match (ä¸€å¿ƒä¸€æ„)
4. ABCC - Third char repeats as fourth (å–œæ°”æ´‹æ´‹)
5. AABC - First char repeats with two different chars (æ´¥æ´¥æœ‰å‘³)
6. ABCB - Middle char repeats (å¿ƒæœå£æœ)

#### 3. Word Processing Integration
- Added `detect_structure()` call in three places:
  - `_load_idioms()` - Chinese idioms
  - `_load_words()` - Regular words
  - `_load_xiehouyu()` - Riddle answers
- All structure field assignments changed from `None` to `structure` variable

#### 4. Testing Results
All 6 plan examples pass:
- âœ“ é«˜é«˜å…´å…´ (AABB)
- âœ“ ç ”ç©¶ç ”ç©¶ (ABAB)
- âœ“ ä¸€å¿ƒä¸€æ„ (ABAC)
- âœ“ å–œæ°”æ´‹æ´‹ (ABCC)
- âœ“ æ´¥æ´¥æœ‰å‘³ (AABC)
- âœ“ å¿ƒæœå£æœ (ABCB)

#### 5. Code Organization
- Created `lexicon/structure.py` as a focused module for structure detection
- Exported `STRUCTURE_PATTERNS` dict for reference
- Exported `detect_structure()` function used by SearchEngine
- Follows single responsibility principle

### Files Modified
1. `lexicon/structure.py` - NEW: Structure detection implementation
2. `lexicon/search.py` - UPDATED: Import and use detect_structure() in three load methods

## P2 Advanced Search Features Implementation

### Completed Features (All 4 P2 Features)

#### 1. **Pattern/Wildcard Search** (`--pattern`)
- Converts wildcard patterns to regex: `?` â†’ `.` (any char), `*` â†’ `.*` (any chars)
- Uses `^pattern$` anchors for exact word matching
- Example: `--pattern "ä¸€?ä¸€?"` finds "ä¸€å¿ƒä¸€æ„", "ä¸€ç¬‘ä¸€é¢¦", etc.
- Implementation: `pattern.replace("?", ".").replace("*", ".*")`

#### 2. **Structure Search** (`--structure`)
- Leverages existing `index.by_structure` dictionary
- Already supports AABB, ABAB, ABAC, ABCC, AABC, ABCB patterns
- Example: `--structure AABB` finds "å®¶å®¶æˆ·æˆ·", "åååå", etc.
- Uses fast index lookup, no regex scanning needed

#### 3. **Rhyme Search** (`--rhyme`)
- Matches by final/rhyme (éŸµæ¯) of last character
- Uses `index.by_rhyme` dictionary
- Example: `--rhyme ang` finds all words ending with "ang" rhyme
- Already computed during word loading via `get_rhyme()`

#### 4. **Tone Search** (`--tone`)
- Matches tone sequences as stored in `word.tones`
- Format: comma-separated tone numbers like "1,2,3,4"
- Example: `--tone "1,2,3,4"` finds "æ’‘è‚ æ‹„è…¹", "å‡ºè¨€åæ°”", etc.
- Direct string comparison after extracting tones from pinyin

#### 5. **Homophone Search** (`--homophone`)
- Finds words with same `pinyin_no_tone` as input word
- Uses `get_pinyin_no_tone()` to normalize search term
- Example: `--homophone ä¸­` finds "ä¼—", "å¦•", "èŒ½" (all pronounced "zhong")
- Works by comparing `word.pinyin_no_tone` strings

### Search Engine Enhancements (`lexicon/search.py`)

1. **New Parameters Added to `search()` Method**:
   - `pattern: str | None` - Wildcard pattern
   - `structure: str | None` - Structure pattern
   - `rhyme: str | None` - Rhyme/final
   - `tone: str | None` - Tone sequence
   - `homophone: str | None` - Homophone search

2. **Filter Implementation Order**:
   - Category â†’ Length â†’ Pinyin â†’ Full Pinyin â†’ Start â†’ End â†’ Contains â†’ Structure â†’ Rhyme
   - Pattern â†’ Tone â†’ Homophone â†’ Regex
   - Most selective filters first for performance

3. **Early Exit Strategy**: Returns empty list if any filter produces no candidates

### CLI Enhancements (`lexicon/cli.py`)

1. **New Typer Options Added**:
   ```python
   --pattern TEXT        # é€šé…ç¬¦æ¨¡å¼ - Wildcard pattern
   --structure TEXT      # ç»“æ„ - Structure (AABB/ABAC/etc)
   --rhyme TEXT         # éŸµæ¯ - Rhyme/final
   --tone TEXT          # å£°è°ƒæ¨¡å¼ - Tone sequence (1,2,3,4)
   --homophone TEXT     # åŒéŸ³è¯ - Homophone word
   ```

2. **All New Parameters Passed to SearchEngine**:
   - CLI properly forwards all 5 new parameters
   - Output formatting unchanged (backward compatible)

### Key Design Decisions

1. **Index-Driven Approach**: All features leverage existing indexes for performance
2. **Regex Pattern Conversion**: Wildcard patterns converted to regex with anchors for accuracy
3. **Direct String Matching**: Tone and rhyme use simple dictionary lookups, no regex
4. **Backward Compatible**: All existing features continue to work unchanged

### Testing Results

âœ“ Pattern search: `--pattern "ä¸€?ä¸€?"` â†’ finds ABAC pattern words
âœ“ Structure search: `--structure AABB` â†’ finds reduplication patterns
âœ“ Rhyme search: `--rhyme ang` â†’ finds words by final rhyme
âœ“ Tone search: `--tone "1,2,3,4"` â†’ finds by tone sequence
âœ“ Homophone search: `--homophone ä¸­` â†’ finds same-pronunciation words
âœ“ Backward compatibility: All existing search features work unchanged
âœ“ CLI help: All new options display correctly with bilingual descriptions

### Performance Characteristics

- Pattern search: O(n) regex scan through candidates
- Structure search: O(1) index lookup
- Rhyme search: O(1) index lookup
- Tone search: O(n) linear scan through candidates
- Homophone search: O(n) linear scan through candidates
- Combined filters: Efficient candidate set intersection

## Freq Command Implementation (2025-02-01)

### Successfully Implemented
Added new `freq` command to `lexicon/cli.py` for character frequency statistics.

### Command Specification
```bash
lexicon freq [OPTIONS]
  --position TEXT   Position: start/end/all [default: all]
  --limit INTEGER   Number of results [default: 20]
```

### Implementation Details

1. **Position Parameter Options**:
   - `start` - Frequency of first characters (é¦–å­—) using `index.char_freq_start`
   - `end` - Frequency of last characters (å°¾å­—) using `index.char_freq_end`
   - `all` - Frequency of all characters (å…¨éƒ¨) using `index.char_freq_all`
   - Default: `all`

2. **Limit Parameter**:
   - Number of top characters to display
   - Default: 20
   - Uses `Counter.most_common(limit)` for efficient retrieval

3. **Output Format**:
   - Header: `ğŸ“Š å­—é¢‘ç»Ÿè®¡ (é¦–å­—):`, `ğŸ“Š å­—é¢‘ç»Ÿè®¡ (å°¾å­—):`, or `ğŸ“Š å­—é¢‘ç»Ÿè®¡ (å…¨éƒ¨):`
   - Each character on its own line: `   å­—: count` (3 spaces indent)
   - Trailing newline for clean output

4. **Error Handling**:
   - Validates position parameter (case-insensitive)
   - Shows helpful error message with valid options if invalid
   - Handles empty frequency data gracefully

### Example Output

```bash
$ lexicon freq --position start --limit 10
ğŸ“Š å­—é¢‘ç»Ÿè®¡ (é¦–å­—):
   ä¸€: 851
   ä¸: 618
   å¤§: 543
   è€: 437
   ä¸‰: 346
   æ— : 335
   ä»¥: 230
   é£: 221
   å¤©: 217
   é»„: 214
```

### Data Source
Leverages existing `LexiconIndex` frequency counters:
- `char_freq_start` - Counter of first characters across all words
- `char_freq_end` - Counter of last characters across all words  
- `char_freq_all` - Counter of all characters in all words

These counters are built during `SearchEngine.__init__()` when the index is constructed.

### Testing Results
âœ… Default parameters: `lexicon freq` â†’ Shows top 20 all characters
âœ… Start position: `lexicon freq --position start --limit 10` â†’ Works correctly
âœ… End position: `lexicon freq --position end --limit 10` â†’ Works correctly
âœ… All position: `lexicon freq --position all --limit 10` â†’ Works correctly
âœ… Case insensitivity: `lexicon freq --position START` â†’ Works correctly
âœ… Invalid position: `lexicon freq --position invalid` â†’ Shows helpful error
âœ… Help system: `lexicon freq --help` â†’ Displays command documentation

### Integration
- Command follows existing CLI patterns (lazy SearchEngine loading)
- Uses same error message formatting as other commands
- Properly integrated into typer app with `@app.command()` decorator
- Full help text automatically generated by typer

### Code Quality
- Removed all unnecessary inline comments (self-documenting code)
- Kept essential public API docstring for CLI help
- Clean variable names and simple logic flow
- Proper error handling and validation

## Fill Command Implementation (2025-02-01)

### Successfully Implemented
Added new `fill` command to `lexicon/cli.py` for interactive fill-in-the-blank word game (å¡«å­—æ¸¸æˆ).

### Command Specification
```bash
lexicon fill PATTERN [OPTIONS]
  PATTERN              å¡«å­—æ¨¡å¼ï¼Œä½¿ç”¨ ? è¡¨ç¤ºå¾…å¡«å­—ç¬¦ (å¦‚: ä¸€?ä¸€?)
  --category, -c TEXT  ç±»å‹ (æˆè¯­/è¯è¯­/æ­‡åè¯­)
  --limit INTEGER      æœ€å¤§ç»“æœæ•°é‡ [default: 20]
```

### Game Mechanics - 4 Distinct Modes

1. **No Matches** (0 results)
   - Output: "æœªæ‰¾åˆ°åŒ¹é…çš„è¯è¯­"
   - Exit gracefully with code 0

2. **Single Match** (1 result)
   - Shows answer directly with emoji: âœ¨ æ‰¾åˆ°ç­”æ¡ˆï¼šè¯è¯­
   - Displays: pinyin, definition
   - Non-interactive mode

3. **Game Mode** (2-10 matches)
   - Interactive guessing game
   - Shows: ğŸ¯ å¡«å­—æ¸¸æˆ
   - Displays: pattern, definition of random selected word
   - User gets 2 attempts to guess
   - After first wrong guess: hint showing first and last character
   - Feedback: âœ… æ­£ç¡®ï¼or "æ¸¸æˆç»“æŸï¼ç­”æ¡ˆæ˜¯ã€Œè¯è¯­ã€"

4. **List All** (10+ matches)
   - Shows all matching results with numbering
   - Format: "N. word [pinyin] - definition"
   - Ordered list display

### Implementation Details

1. **Pattern Validation**:
   - Validates pattern not empty
   - Requires at least one `?` character
   - Error: "âŒ é”™è¯¯ï¼šæ¨¡å¼å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ª ? å­—ç¬¦"

2. **Search Integration**:
   - Reuses existing `SearchEngine.search(pattern=pattern, category=category, limit=limit)`
   - Pattern converted to regex internally: `?` â†’ `.`, `*` â†’ `.*`

3. **Game Logic**:
   - Uses `random_module.choice()` to pick random word from matches
   - `max_attempts = 2` for guessing
   - Tracks attempt count for hint generation
   - First hint (after wrong answer): displays first and last character

4. **Key Features**:
   - Category filter support (æˆè¯­/è¯è¯­/æ­‡åè¯­)
   - Customizable result limit
   - Graceful keyboard interrupt handling (Ctrl+C)
   - Proper exit codes (0 for success, 1 for errors)
   - Chinese error messages matching other commands

### Code Organization

```python
@app.command()
def fill(
    pattern: str = typer.Argument(..., help="å¡«å­—æ¨¡å¼ï¼Œä½¿ç”¨ ? è¡¨ç¤ºå¾…å¡«å­—ç¬¦"),
    category: Optional[str] = typer.Option(None, "--category", "-c", help="ç±»å‹"),
    limit: int = typer.Option(20, "--limit", help="æœ€å¤§ç»“æœæ•°é‡")
) -> None:
    """å¡«å­—æ¸¸æˆ - Fill-in-the-blank word game."""
```

### Testing Results
âœ… Help system: `lexicon fill --help` â†’ Displays command documentation
âœ… No pattern: `lexicon fill ""` â†’ Error exit code 1
âœ… No question mark: `lexicon fill "ä¸­å›½"` â†’ Error exit code 1
âœ… No matches: `lexicon fill "é‡‘?é‡‘?"` â†’ Shows "æœªæ‰¾åˆ°åŒ¹é…çš„è¯è¯­"
âœ… One match: `lexicon fill "åˆ«æœ‰?å¤©"` â†’ Shows answer with âœ¨
âœ… Multiple matches (2-10): `lexicon fill "ä¸€?ä¸€?"` â†’ Game mode with ğŸ¯
âœ… Many matches (10+): `lexicon fill "?å¿ƒ?æ„" --limit 20` â†’ Lists all results
âœ… Category filter: Works with --category æˆè¯­/è¯è¯­/æ­‡åè¯­
âœ… Game mode interaction: Correct guess â†’ âœ… æ­£ç¡®ï¼
âœ… Game mode wrong guess: Shows hints and attempt counter
âœ… All 49 CLI tests pass (39 original + 10 new fill tests)

### Test Coverage

1. **Unit Tests** (10 new tests):
   - Pattern validation (empty, no ?)
   - Match count behavior (0, 1, 2-10, 10+)
   - Category filtering
   - Limit validation
   - Game mode with correct/wrong answers
   - Input handling and hints

2. **Test Fixtures**:
   - `sample_words` - Original fixture with 3 words (for backward compatibility)
   - `sample_words_for_fill` - NEW: Extended fixture with 5 words for fill tests
   - `mock_search_engine_for_fill` - NEW: Dedicated fixture for fill command tests

### Integration Points

1. **SearchEngine**: Reuses existing pattern matching via `engine.search(pattern=...)`
2. **CLI Framework**: Follows existing command patterns (typer @app.command)
3. **Random Selection**: Uses `random_module.choice()` from standard library
4. **Error Handling**: Matches existing error message formatting and logging

### Example Usage

```bash
# Single match - direct answer
$ lexicon fill "åˆ«æœ‰?å¤©"
âœ¨ æ‰¾åˆ°ç­”æ¡ˆï¼šåˆ«æœ‰æ´å¤©
   æ‹¼éŸ³: biÃ© yÇ’u dÃ²ng tiÄn
   é‡Šä¹‰: æ´ä¸­å¦æœ‰ä¸€ä¸ªå¤©åœ°ã€‚å½¢å®¹é£æ™¯å¥‡ç‰¹ï¼Œå¼•äººå…¥èƒœã€‚

# Multiple matches - game mode
$ lexicon fill "ä¸€?ä¸€?" 
ğŸ¯ å¡«å­—æ¸¸æˆ

æ¨¡å¼: ä¸€?ä¸€?
é‡Šä¹‰: æŒ‡è„¸ä¸Šçš„è¡¨æƒ…ã€‚åŒä¸€é¢¦ä¸€ç¬‘"ã€‚

ä½ çš„ç­”æ¡ˆ (è¿˜æœ‰ 2 æ¬¡æœºä¼š) []: ä¸€ç¬‘ä¸€é¢¦
âŒ ä¸å¯¹ï¼
   æç¤º: é¦–å­—æ˜¯ã€Œä¸€ã€; å°¾å­—æ˜¯ã€Œå¾·ã€

ä½ çš„ç­”æ¡ˆ (è¿˜æœ‰ 1 æ¬¡æœºä¼š) []: ä¸€å¿ƒä¸€å¾·
âœ… æ­£ç¡®ï¼

# Many matches - list all
$ lexicon fill "?å¿ƒ?æ„" --limit 20
æ‰¾åˆ° 20 æ¡ç»“æœ:
1. ä¸€å¿ƒä¸€æ„ [yÃ¬ xÄ«n yÃ­ yÃ¬] - åªæœ‰ä¸€ä¸ªå¿ƒçœ¼å„¿ï¼Œæ²¡æœ‰åˆ«çš„è€ƒè™‘ã€‚
2. ä¸‰å¿ƒäºŒæ„ [sÄn xÄ«n Ã¨r yÃ¬] - åˆæƒ³è¿™æ ·åˆæƒ³é‚£æ ·ï¼ŒçŠ¹è±«ä¸å®šã€‚
3. å¥½å¿ƒå¥½æ„ [hÇo xÄ«n hÇo yÃ¬] - æŒ‡æ€€ç€å–„æ„ã€‚
...
```

### Files Modified
1. `lexicon/cli.py` - UPDATED: Added fill command (123 lines of code)
2. `tests/test_cli.py` - UPDATED: Added 10 new fill command tests

### Design Decisions

1. **2 Attempts vs 3**: Chosen 2 attempts (not 3 like quiz) to match "fill-in-the-blank" nature where users already see the structure
2. **Hint Strategy**: First hint shows first and last character (structural clues) 
3. **Match Thresholds**: 2-10 = game (engaging), 10+ = list (overwhelming to guess)
4. **Pattern Validation**: Require `?` to prevent accidental full-word searches
5. **Category Optional**: Allows broader searches without category filtering

### Performance
- Pattern matching: O(n) through candidates
- Random selection: O(1) 
- Overall complexity: O(n) where n = number of words after filters applied
- Efficient for typical use cases (<100 matches)

